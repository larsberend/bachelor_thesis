\documentclass[Bachelorarbeit.tex]{subfiles}
\begin{document}

\newpage
\section{Optical Flow}
\label{Optical Flow}
In this section, optical flow in the field of computer vision is defined and problems, which arise when trying to calculate it, are shown. Optical flow is the apparent velocity of everything visible in a scene caused by the relative motion of observer and the scene, and can therefore yield important information about movement, the location of objects and overall changes in the scene \citep[p. 185]{horn1981determining}. For classifying eye-movement, it is important to receive more information about motion in the image sequence.
%$But it is to be remembered, that the relation of optical flow to motion of objects is not always obvious \citep[p. 186]{horn1981determining}. For example 
To obtain this, the general way to calculate optical flow of consecutive frames in a video is explained and the aperture problem is shown. Following is the method to solve these problems used in this thesis, the Lucas-Kanade method. It includes the neighborhood of a pixel in the calculation.


\subsection{Calculating Optical Flow}
To determine optical flow in image sequences in a video, one has to calculate the velocity of a 3D world on a 2D image plane. Take the human visual system as an example, in which the retina is the 2D image plane. By using different cues of the 3D world humans exist in, first and foremost stereo vision, computation of optical flow and therefore the movement in the 3D world, is possible.
Optical flow can be expressed in a mathematical formalism, which will be explained in the following. First, the following constraint is assumed: The brightness of a pixel does not change when moving through time an space \citep[p.187]{horn1981determining}. 
Under this assumption the movement of a pixel in a video can be viewed as follows:
\begin{equation}
E(x,y,t) = E(x + \delta x, y + \delta y, t + \delta t)
\end{equation}
With E(x,y,t) being the brightness of a pixel at location (x,y) in the image plane at time t and $\delta x$, $\delta y$ and $\delta t$ referring to the change in these three dimensions. \\ \\
Now, the Taylor series can be used to expand the right hand side:
\begin{equation*}
E(x,y,t) = E(x,y,t) + \delta x \frac{\partial E}{\partial x} + \delta y \frac{\partial E}{\partial y} + \delta t \frac{\partial E}{\partial t} + \epsilon
\end{equation*}
\\This formula can be rearranged to\\
\begin{equation*}
\delta x \frac{\partial E}{\partial x} + \delta y \frac{\partial E}{\partial y} + \delta t \frac{\partial E}{\partial t} + \epsilon = 0
\end{equation*}
Where the partial derivatives ($\partial$) contain the gradient information in x, y and t direction, respectively.\\
\\Division by $\delta t$ yields:\\
\begin{equation*}
\frac{\delta x}{\delta t} \frac{\partial E}{\partial x} + \frac{\delta y}{\delta t} \frac{\partial E}{\partial y} + \frac{\partial E}{\partial t} + \displaystyle{\lim_{\epsilon \to 0}} = 0 
\end{equation*}
Given $u = \frac{\delta x}{\delta t}$ and $v = \frac{\delta y}{\delta t}$,the \textit{optical flow equation} can be derived:
\begin{equation}
\label{optfloweq}
E_x u + E_y v + E_t = 0
\end{equation}
The problem of determining optical flow becomes apparent here, a single equation with two unknown variables $u$ and $v$ is unsolvable. This is also called the aperture problem of optical flow. Semantically this problem is comparable to a human who lost one of his eyes and therefore struggles to derive motion information correctly. Several methods to work around this problem exist, one of which will be discussed in the following.

\subsection{Lucas Kanade Method}
\label{lk}
To solve the aperture problem \cite{lucas1981iterative} introduced an additional constraint: The pixels in the neighborhood of a pixel are assumed to move identically to it. In this way, \autoref{optfloweq} becomes solvable, since we can solve for $u$ and $v$, with 5 equations given a 4-neighborhood or 9 equations given a 8-neighborhood for a target pixel. \cite{lucas1981iterative} apply the least-squares method to solve this overdetermined system of equations for $u$ and $v$. 
In other words, to calculate the location of a part of a 3D world, depicted by pixel $A_t$ and its neighborhood at time $t$, their intensity is compared with the same pixel $A_{t+\delta t}$ and its neighborhood at the next timepoint $t+\delta t$, now depicting a different part of the world. Using the gradients in the neighborhood of the point it is possible to estimate the velocity of movement, $u$ and $v$, and therefore the new location of the point in space at pixel $A'_{t+\delta t}$ \citep{rojas2010lucas}. \\
Problems with this method exist. Firstly, in a real-world scenario, pixel intensity can change because of difference in lighting or noise. This can lead to inconsistencies of the neighborhood constraint. Secondly, occlusion and therefore disappearing points in space do pose a problem in all movement detection applications. In this project this problem occurs particularly, since subjects blink during eye-tracking recordings. \todo{blinks?} Last but not least, this method is purely local, which means that only small patches of the frames (neighborhood) are taken into account and not the whole image. Therefore, only small motions relative to the framerate are tracked. \\
Though these problems occur, with certain adaptions of the algorithm it proves to be the one best performing optical flow methods (see \cite{galvin1998recovering} and \cite{barron1992performance}).
\\One of these adaptions is the removal of noise by a suppression of high spatial frequencies. Noise is changing from frame to frame and can lead to false movement detection.
However, this low-pass filtering also suppresses small details and a trade-off has to be found \citep[p. 123]{lucas1981iterative}, more on that in \autoref{preprocessing}
To account for the ability to track large (relative to image size) or fast (relative to frame rate) motions, both starting image and consecutive images can be reduced by downsampling. This transform summarizes a portion of neighboring pixels via interpolation and yields an image with a smaller resolution. This step can be repeated to estimate every size of movement, without changing much of the actual Lucas-Kanade algorithm \cite{bouguet2001pyramidal}. As an example one can imagine two balls rolling over the ground, one fast and one slow. The slow one is rolling in the video at a speed of 5 pixels per frame. The fast one at 10 pixel per frame. To detect movement in the image, a point of both balls is tracked with Lucas-Kanade optical flow. Because the detection in this example is only possible in a small neighborhood of 10 pixels, the slow ball can be tracked, but the fast ball is not in a region of search for the algorithm. Now, downsampling takes place. Since the balls are much brighter than the dark background in the videos, this reduction of resolution does not pose problems for the detection of the balls and the fast ball is now moving only with a speed of 8 pixels per second. Now the Lucas-Kanade method can track this ball as well.\\
One can choose to determine the optical flow of every pixel in the image or use a number of key points to determine it in salient regions of the image, also called dense and sparse optical flow, respectively. In this thesis, the latter is being used, because not every pixel is valuable or useful for tracking, ergo computational time can be saved \citep{shi1993good}. Key points are also called salient points, features or regions of interest. The goal of their detection is to determine points in images which are 'important', differentiable or discriminative in their neighborhood and therefore can characterize the image as a whole or parts of it. Key point detection is an essential part of computer vision research and not only so for motion analysis, but also for object detection, camera calibration, 3D reconstruction and pose estimation \citep[p. 179]{tuytelaars2008local}. In order to achieve the best results with the Lucas-Kanade algorithm, the key point detection has to be optimized. For this reason, multiple key point detection algorithms can be compared and optimised.A vast amount of detection algorithms exist and can be grouped roughly in the following classes, as was done by \cite[pp. 337]{gauglitz2011evaluation}:\\ 
\begin{itemize}
	\item Corner detectors compare candidates for interest points to their neighbors. It becomes apparent, that a strong local extrema in x-direction yields pixel laying on an edge. If this is accompanied by an extrema in y-direction as well, one can see this as a corner, which are regions that have been viewed as interesting for a long time\citep[p. 337]{gauglitz2011evaluation}.
	\item Blob detectors use extrema of various filters to determine if a region is of interest for further analysis. These regions can be ascribed to an anchor point, most often the center pixel, to yield key points \citep[p. 338]{gauglitz2011evaluation}.
	\item Affine-Invariant detectors have been proposed to yield key points, which are robust to affine changes. These changes include rotation, translation and scaling. Most of these detectors have high computational cost.
\end{itemize}

 

\FloatBarrier
\end{document}